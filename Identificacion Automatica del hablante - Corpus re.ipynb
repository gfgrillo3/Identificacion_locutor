{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "satisfied-messaging",
   "metadata": {},
   "source": [
    "# Introducción"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seasonal-rough",
   "metadata": {},
   "source": [
    "Desarrollo de un modelo de Machine Learning para la identificación automática del hablante.\n",
    "\n",
    "Se Creó una red Perceptrón Multi Capa, modelando lo planteado en el paper: Identificación automática del hablante mediante redes neuronales de la UNER\n",
    "\n",
    "Link: https://www.researchgate.net/publication/265964745_Identificacion_Automatica_del_Hablante_mediante_Redes_Neuronales\n",
    "        \n",
    "Torres, Humberto & Rufiner, Hugo. (1999). Identificación Automática del Hablante mediante Redes Neuronales."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "laughing-miller",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extensive-position",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "joint-twenty",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bef5548",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy==1.20.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "architectural-insertion",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import scikitplot as skplt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import librosa\n",
    "from librosa import display\n",
    "from joblib import load, dump\n",
    "from datetime import datetime\n",
    "import math\n",
    "from google.cloud import storage\n",
    "from google.oauth2 import service_account\n",
    "from project_lib import Project\n",
    "import json\n",
    "import warnings\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2c5124",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -Iv scikit-learn==1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6011f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0f56e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "from google.oauth2 import service_account"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d33088",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "domestic-occasions",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Configuración general de visualización del notebook\n",
    "\n",
    "#Seteo para ver todas las columnas cuando muestro un dataframe\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "#Seteo para que no muestre los Warnings de valores asignados en un slice de una copia de un DF. (Simplifica lectura de Logs)\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab1475c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ME CONECTO AL PROYECTO PARA PODER HACER LA LECTURA DE LOS ARCHIVOS QUE TENGO GURDADOS DENTRO\n",
    "\n",
    "#Token Entorno\n",
    "project = Project(project_id='821deb35-d3d9-4a8d-a5e7-b1823688b22f', \n",
    "                  project_access_token='ACCESS_TOKEN')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fresh-inflation",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conscious-carolina",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "encouraging-classroom",
   "metadata": {},
   "source": [
    "# Variables Globales"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "streaming-accommodation",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "harmful-soviet",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cantidad de personas usadas en el entrenamiento\n",
    "cant_personas_train = 8\n",
    "\n",
    "#Cantidad de Coeficientes MFCC a extraer(incluidos DELTAS)\n",
    "cant_MFCC = 32\n",
    "\n",
    "#Nombre del archivo donde se guarda el usuario a entrenar\n",
    "user_to_train_filename = \"user_to_train.txt\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a408a99f",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37e5076",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "relevant-registration",
   "metadata": {},
   "source": [
    "# DEFINICIÓN DE FUNCIONES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oriental-scanner",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Notar que por default, se establecen los valores:\n",
    "\n",
    "#sr=16000. Tasa de muestreo de los audios de TIMIT\n",
    "\n",
    "#n_mfcc=16. Cantidad de Coeficientes utilizados en el Paper a replicar\n",
    "\n",
    "#n_fft=512. Tamaño de la ventana en la transformada de Fourier. (Notar que, sin solapamiento,\n",
    "#se obtendrán 16000/512 frames por segundo)\n",
    "\n",
    "#hop_length=256. Tamaño del salto. Con 256, se logra un solapamiento del 0.5 entre cada ventana de la FFT, logrando así\n",
    "#captar características propias de los pronunciamientos y fonemas.\n",
    "\n",
    "\n",
    "\n",
    "def loadMfccArchivo(archivoVoz, sr=16000, n_mfcc=16, n_fft=512, hop_length=256):\n",
    "    \"\"\"\n",
    "    Carga de los coeficientes Cepstrum en la escal Mel de un archivo utilizando la librería librosa.\n",
    "    \n",
    "    \n",
    "    \n",
    "    Params:\n",
    "    \n",
    "    archivoVoz = archivo a procesar\n",
    "    sr = Sampling Rate del archivo (en Hz)\n",
    "    n_mfcc = cantidad de coeficientes cepstrales a extraer\n",
    "    n_fft = tamaño de la ventana de la transformada de Fourier\n",
    "    hop_length = tamaño del salto para el procesamiento en frames\n",
    "    \n",
    "    \n",
    "    \n",
    "    Para mas información:\n",
    "    \n",
    "    MFCC = https://es.wikipedia.org/wiki/MFCC\n",
    "    \n",
    "    Librosa MFCC = https://librosa.org/doc/main/generated/librosa.feature.mfcc.html\n",
    "    \"\"\"\n",
    "    y,sr=librosa.load(archivoVoz, sr=sr)\n",
    "    return librosa.feature.mfcc(y, sr=sr, n_mfcc=n_mfcc, dct_type=2, norm='ortho',\n",
    "                                hop_length=hop_length, n_fft=n_fft) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "private-shelter",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brilliant-shame",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Funcion para agregar en una matriz, un vector de identificacion.\n",
    "#El vector identifica, para el i-ésimo vector de la matriz de mfcc, a que persona corresponde dicho MFCC.\n",
    "\n",
    "#Agrega a la matriz un vector de longitud K, donde K son la cant. personas a identificar, \n",
    "#siendo todas las posiciones 0, salvo la persona a la que corresponde el frame que es 1.\n",
    "\n",
    "#I.E. Siendo K=5 personas a identificar, la persona 3 identificada en el frame, se agregará al vectorIdentificados\n",
    "#el frame actual, y a personaIdentificada el array (0,0,1,0,0)\n",
    "\n",
    "#A efectos de replicar el paper, se establece como default la cantidad de personas en 8, en caso de testear con menos\n",
    "#se deja a libre elección el parámetro\n",
    "\n",
    "\n",
    "def agregarIdentificacionPersona(personaIdentificada, vectorIdentificados, cantPersonas=8):\n",
    "    \n",
    "    \"\"\"\n",
    "    Agregado de la identificación de una persona a un vector de personas identificadas, y del frame actual\n",
    "    a un vector con frames de audios\n",
    "    \n",
    "    Agregado en el final, para no perder referencia de que el n-ésimo frame del vector de frames identificados\n",
    "    corresponde a la persona marcada con 1 en el n-ésimo frame del vector de personas identificadas\n",
    "    \n",
    "    Params:\n",
    "    \n",
    "    personaIdentificada = número de la persona identificada\n",
    "    vectorIdentificados = vector de personas identificadas hasta el momento\n",
    "    cantPersonas = cantidad de personas a analizar pertenencia\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    #Creo un vector de todos zeros. Luego, con índice comenzando en 0, marco a la persona identificada con 1\n",
    "    #apilo verticalmente al final, y retorno el vector con la persona nueva identificada\n",
    "    \n",
    "    vectorCeros = np.zeros((cantPersonas,))              \n",
    "    vectorCeros[personaIdentificada-1] = 1\n",
    "    vectorIdentificados = np.vstack((vectorIdentificados, vectorCeros))\n",
    "    return vectorIdentificados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff2e26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def guardar_output_como_pickle(filename, model):\n",
    "    \n",
    "    \n",
    "    with open(filename, 'wb+') as z:\n",
    "        \n",
    "        data = pickle.dumps(model)\n",
    "\n",
    "        project.save_data(filename, data, set_project_asset=True, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce971406",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cargar_modelo_pickle(filename):\n",
    "    \n",
    "    modelo = pickle.load(project.get_file(filename))\n",
    "    \n",
    "    return modelo\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "negative-criterion",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d1a922",
   "metadata": {},
   "source": [
    "# Usuario a entrenar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7e1887",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ASI SE CARGA UN TXT\n",
    "user_to_train = project.get_file(user_to_train_filename).read().decode('UTF-8').upper()\n",
    "print(\"Se entrenará el modelo para \"+user_to_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4492a64",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa27725",
   "metadata": {},
   "source": [
    "# Carga de audios de firebase"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd2cf28",
   "metadata": {},
   "source": [
    "Desde Firebase se leen los audios y se separan entre 8 personas aleatorias con las que se entrenará el modelo, y las que el modelo NO conoce (impostores).\n",
    "\n",
    "\n",
    "Con todas ellas luego se evaluará la performance del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d9c2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creamos las credenciales con las que nos conectamos a la cuenta de servicio para obtener los archivos\n",
    "#Creamos el cliente del storage para luego utilizar las funcionalidades\n",
    "#obtenemos los buckets y los imprimimos para ver que no haya cambios\n",
    "\n",
    "credentials_file_name = \"cuidartech-7e54f-firebase-adminsdk-8qlnh-6b3bfdfa23.json\"\n",
    "\n",
    "json_credentials = json.load(project.get_file(credentials_file_name))\n",
    "\n",
    "credentials = service_account.Credentials.from_service_account_info(json_credentials)\n",
    "\n",
    "storage_client = storage.Client(credentials=credentials)\n",
    "\n",
    "buckets = list(storage_client.list_buckets())\n",
    "print(buckets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b71dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creamos la instancia del bucket en donde tenemos los archivos y luego listamos todos los blobs\n",
    "\n",
    "bucket = storage_client.get_bucket(\"cuidartech-7e54f.appspot.com\")\n",
    "\n",
    "blobs = list(bucket.list_blobs(prefix=\"audios/\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b49c1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "blobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a4a891",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Nos quedamos con todos los usuarios que tenemos audios de ellos, y sus nombres únicos.\n",
    "#Ordenamos aleatoriamente para entrenar al modelo con personas aleatorias\n",
    "#Una vez ordenado, nos quedamos con los primeros 8\n",
    "\n",
    "users = [ blob.name.split('/')[1].upper() for blob in blobs]\n",
    "\n",
    "users = np.unique(users)\n",
    "\n",
    "np.random.shuffle(users)\n",
    "\n",
    "users = users.tolist()\n",
    "\n",
    "users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e48bb24",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creamos los usuarios con los que se entrenará el modelo.\n",
    "#En primer lugar eliminamos al usuario que queremos entrenar de la lista de todos los usuarios, agregamos 7 usuarios al azar de\n",
    "#esta lista para entrenar al modelo, y agregamos como octavo al que se quiere identificar, es decir, para el que voy a entrenar.\n",
    "\n",
    "\n",
    "users.remove(user_to_train)\n",
    "\n",
    "\n",
    "used_users = users[:7]\n",
    "\n",
    "used_users.append(user_to_train)\n",
    "\n",
    "\n",
    "np.random.shuffle(used_users)\n",
    "\n",
    "\n",
    "used_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d8bcaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Nos quedamos con los usuarios no utilizados para probar impostores\n",
    "\n",
    "not_used_users = users[7:]\n",
    "not_used_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418ece63",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creamos los 3 diccionarios a usar. Uno con todos los audios y luego los de train y tests.\n",
    "\n",
    "diccionario_audios_used_users = dict.fromkeys(used_users, [])\n",
    "\n",
    "diccionario_audios_train = dict.fromkeys(used_users, [])\n",
    "\n",
    "diccionario_audios_test = dict.fromkeys(used_users, [])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c281da28",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creamos el diccionario de los audios de los impostores\n",
    "\n",
    "diccionario_audios_not_used_users = dict.fromkeys(not_used_users, [])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf8d06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Limpiamos los diccionarios para que cada key haga referencia a un array diferente\n",
    "\n",
    "for user_to_clean in diccionario_audios_used_users.keys():\n",
    "    diccionario_audios_used_users[user_to_clean] = []\n",
    "    diccionario_audios_train[user_to_clean] = []\n",
    "    diccionario_audios_test[user_to_clean] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a1b6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Limpiamos los diccionarios para que cada key haga referencia a un array diferente\n",
    "\n",
    "for user_to_clean in diccionario_audios_not_used_users.keys():\n",
    "    diccionario_audios_not_used_users[user_to_clean] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e48b102",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#De todos los blobs que tenemos cargamos los nombres de blobs de los usuarios que actualmente vamos a entrenar.\n",
    "#Una vez que los tenemos, vamos a guardar en el diccionario donde tenemos los nombres de los blobs de todos los audios\n",
    "\n",
    "for blob in blobs:\n",
    "    \n",
    "    actual_user = blob.name.split('/')[1].upper()\n",
    "    \n",
    "    if actual_user in used_users:\n",
    "        diccionario_audios_used_users[actual_user].append(blob.name)\n",
    "    \n",
    "    elif actual_user in not_used_users:\n",
    "        diccionario_audios_not_used_users[actual_user].append(blob.name)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a6767c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vamos a separar los audios en train y test.\n",
    "#Recorremos el diccionario con todas las personas y audios, para cada persona, los primeros 3 audios que encontremos van a train y el resto a test\n",
    "\n",
    "for persona in diccionario_audios_used_users.keys():\n",
    "    \n",
    "    audio_actual=0\n",
    "    \n",
    "    \n",
    "    for audio in diccionario_audios_used_users.get(persona):\n",
    "        \n",
    "        audio_actual+=1\n",
    "        \n",
    "        \n",
    "        if audio_actual<=3:\n",
    "            \n",
    "            diccionario_audios_train[persona].append(audio)\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            diccionario_audios_test[persona].append(audio)\n",
    "            \n",
    "\n",
    "    audio_actual=0\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b13eabb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "diccionario_audios_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excess-better",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rotary-template",
   "metadata": {},
   "source": [
    "# CARGA Y ALMACENAMIENTO DE DATOS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rolled-supplier",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "square-special",
   "metadata": {},
   "source": [
    "# INFORMACIÓN DE SET DE DATOS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "psychological-darkness",
   "metadata": {},
   "source": [
    "Se usó un corpus de audios reales grabados de diferentes dispositivos con sistema operativo Android."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electronic-satin",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "finnish-reflection",
   "metadata": {},
   "source": [
    "Los hablantes utilizados son 8 personas de sexo masculino.\n",
    "\n",
    "En total, se poseen 10 archivos de audio por persona, donde se utilizan 3 para entrenamiento, y el restante para verificación.\n",
    "\n",
    "En total: 24 archivos de audio para entrenamiento (3 por persona), y 56 archivos de audio para testeo (7 por persona).\n",
    "\n",
    "Notar que cada archivo de audio es el habla de una única persona.\n",
    "\n",
    "En total, se procesaron 13804 Frames."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dated-struggle",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "least-block",
   "metadata": {},
   "source": [
    "Además, se poseen 4 personas desconocidos para el modelo (impostores), para los cuales se cuentan con 10 audios de cada uno (40 en total).\n",
    "\n",
    "Cabe destacar que para otorgarle un grado extra de complejidad a la identificación del hablante, 1 de los impostores es hijo de 1 de las personas presentes en el modelo.\n",
    "\n",
    "Además, el modelo posee un padre y un hijo con los cuales se entreno. Y a su vez, otra de las personas desconocidas es hermano e hijo de las mencionadas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imperial-upgrade",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "illegal-edgar",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "linear-moses",
   "metadata": {},
   "source": [
    "# PROCESADO DE AUDIOS DE ENTRENAMIENTO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2153373a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#Seteo la primer fila de los MFCC, luego la borro \n",
    "df_MFCC = np.zeros(cant_MFCC,)\n",
    "\n",
    "\n",
    "#Seteo la primer fila de personas, luego la borro\n",
    "df_personas = np.zeros(cant_personas_train,)\n",
    "\n",
    "\n",
    "#Persona de la que leo los features\n",
    "persona_actual = 0\n",
    "\n",
    "\n",
    "#Auxiliar para saber la cantidad de frames totales para train\n",
    "cant_frames_train = 0\n",
    "\n",
    "\n",
    "#Recorro las carpetas de la región del dialecto, donde cada subcarpeta es una persona.\n",
    "for persona in used_users:\n",
    "    \n",
    "    #Persona de la que leo los features\n",
    "    persona_actual += 1\n",
    "\n",
    "    \n",
    "    #Contador de frames leidos de los archivos de la persona actual\n",
    "    cant_frames_persona_actual = 0\n",
    "\n",
    "    \n",
    "    #Carpeta de los archivos de audio de cada persona\n",
    "    for archivo_voz in diccionario_audios_train.get(persona):\n",
    "\n",
    "    \n",
    "        file = open('temp_audio.wav', 'wb+')\n",
    "        bucket.get_blob(archivo_voz).download_to_file(file)\n",
    "        file.close()\n",
    "        \n",
    "        #Extraigo los MFCC de un audio\n",
    "        mfcc = loadMfccArchivo(file.name)\n",
    "        \n",
    "        \n",
    "        #Actualmente en el array tengo n filas x m columnas, donde n es la cantidad de MFCC y m la cantidad de frames\n",
    "        #Traspongo la matriz para quedarme con los features de un determinado frame, es decir n cantidad de MFCC\n",
    "        #columnas x m cantidad de frames filas\n",
    "        mfcc = np.transpose(mfcc)\n",
    "\n",
    "        \n",
    "        #Para agregar información temporal y fonética (velocidad, aceleración, entre otras) a los coeficientes\n",
    "        #agrego los delta de cada feature y los concateno al array en formato de columnas\n",
    "        mfcc = np.append(mfcc, librosa.feature.delta(mfcc), axis = 1)\n",
    "\n",
    "        \n",
    "        #apilo los features de los audios a un array\n",
    "        df_MFCC = np.vstack((df_MFCC, mfcc))\n",
    "\n",
    "        \n",
    "        #Cuento los frames de la persona actual\n",
    "        cant_frames_persona_actual+= mfcc.shape[0]\n",
    "\n",
    "\n",
    "    #Ya lei todos los archivos de la persona\n",
    "    #Paso a identificar la persona para almacenar los frames identificados\n",
    "    for i in range(0, cant_frames_persona_actual):\n",
    "   \n",
    "        df_personas = agregarIdentificacionPersona(persona_actual, df_personas)\n",
    "     \n",
    "    \n",
    "    print(\"Leídos {:d} frames de la persona {:d}\".format(cant_frames_persona_actual, persona_actual))\n",
    "    \n",
    "    cant_frames_train += cant_frames_persona_actual\n",
    "\n",
    "\n",
    "#Borro la primer fila de MFCC que eran todos 0\n",
    "df_MFCC = np.delete(df_MFCC, (0), axis=0)\n",
    "\n",
    "\n",
    "#Borro la primer fila de Personas que eran todos 0\n",
    "df_personas = np.delete(df_personas, (0), axis=0)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b2c71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cant_frames_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "orange-mobility",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wooden-creation",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worldwide-centre",
   "metadata": {},
   "source": [
    "# PROCESADO DE AUDIOS DE TESTEO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "statewide-links",
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#Seteo la primer fila de los MFCC, luego la borro \n",
    "df_MFCC_test = np.zeros(cant_MFCC,)\n",
    "\n",
    "\n",
    "#Seteo la primer fila de personas, luego la borro\n",
    "df_personas_test = np.zeros(cant_personas_train,)\n",
    "\n",
    "\n",
    "#Persona de la que leo los features\n",
    "persona_actual = 0\n",
    "\n",
    "\n",
    "#Auxiliar para saber la cantidad de frames totales para train\n",
    "cant_frames_test = 0\n",
    "\n",
    "\n",
    "#Recorro las carpetas de la región del dialecto, donde cada subcarpeta es una persona.\n",
    "for persona in used_users:\n",
    "    \n",
    "    #Persona de la que leo los features\n",
    "    persona_actual += 1\n",
    "\n",
    "    \n",
    "    #Contador de frames leidos de los archivos de la persona actual\n",
    "    cant_frames_persona_actual = 0\n",
    "\n",
    "    \n",
    "    #Carpeta de los archivos de audio de cada persona\n",
    "    for archivo_voz in diccionario_audios_test.get(persona):\n",
    "\n",
    "    \n",
    "        file = open('temp_audio.wav', 'wb+')\n",
    "        bucket.get_blob(archivo_voz).download_to_file(file)\n",
    "        file.close()\n",
    "        \n",
    "        #Extraigo los MFCC de un audio\n",
    "        mfcc = loadMfccArchivo(file.name)\n",
    "\n",
    "        \n",
    "        #Actualmente en el array tengo n filas x m columnas, donde n es la cantidad de MFCC y m la cantidad de frames\n",
    "        #Traspongo la matriz para quedarme con los features de un determinado frame, es decir n cantidad de MFCC\n",
    "        #columnas x m cantidad de frames filas\n",
    "        mfcc = np.transpose(mfcc)\n",
    "\n",
    "        \n",
    "        #Para agregar información temporal y fonética (velocidad, aceleración, entre otras) a los coeficientes\n",
    "        #agrego los delta de cada feature y los concateno al array en formato de columnas\n",
    "        mfcc = np.append(mfcc, librosa.feature.delta(mfcc), axis = 1)\n",
    "\n",
    "        \n",
    "        #apilo los features de los audios a un array\n",
    "        df_MFCC_test = np.vstack((df_MFCC_test, mfcc))\n",
    "\n",
    "        \n",
    "        #Cuento los frames de la persona actual\n",
    "        cant_frames_persona_actual+= mfcc.shape[0]\n",
    "\n",
    "\n",
    "    #Ya lei todos los archivos de la persona\n",
    "    #Paso a identificar la persona para almacenar los frames identificados\n",
    "    for i in range(0, cant_frames_persona_actual):\n",
    "   \n",
    "        df_personas_test = agregarIdentificacionPersona(persona_actual, df_personas_test)\n",
    "     \n",
    "    \n",
    "    print(\"Leídos {:d} frames de la persona {:d}\".format(cant_frames_persona_actual, persona_actual))\n",
    "    \n",
    "    cant_frames_test += cant_frames_persona_actual\n",
    "\n",
    "\n",
    "#Borro la primer fila de MFCC que eran todos 0\n",
    "df_MFCC_test = np.delete(df_MFCC_test, (0), axis=0)\n",
    "\n",
    "\n",
    "#Borro la primer fila de Personas que eran todos 0\n",
    "df_personas_test = np.delete(df_personas_test, (0), axis=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prime-thanksgiving",
   "metadata": {},
   "outputs": [],
   "source": [
    "cant_frames_totales = cant_frames_test+cant_frames_train\n",
    "print('cant frames test  ',cant_frames_test)\n",
    "print('cant frames train ',cant_frames_train)\n",
    "print('cant frames total ',cant_frames_totales)\n",
    "\n",
    "print('distribucion train {:.2}'.format(cant_frames_train/cant_frames_totales))\n",
    "print('distribucion test  {:.2}'.format(cant_frames_test/cant_frames_totales))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "infrared-sunrise",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complicated-summer",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excited-baltimore",
   "metadata": {},
   "source": [
    "# ENTRENAMIENTO DE LA RED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "formed-corner",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#CREO LA RED CON LAS CARACTERÍSTICAS MENCIONADAS\n",
    "clasificador_voz = MLPClassifier(hidden_layer_sizes=(75, 75), activation='tanh', solver='adam', alpha=0.0001, \n",
    "                            batch_size='auto', learning_rate='adaptive', learning_rate_init=0.001, power_t=0.5, \n",
    "                            max_iter=100000, shuffle=True, random_state=None, tol=0.0001, verbose=True, warm_start=True, \n",
    "                            momentum=0.9, nesterovs_momentum=True, early_stopping=False, validation_fraction=0.1, beta_1=0.9, \n",
    "                            beta_2=0.999, epsilon=1e-08, n_iter_no_change=150, max_fun=15000)\n",
    "\n",
    "\n",
    "\n",
    "#Entreno la red\n",
    "clasificador_voz.fit(df_MFCC, df_personas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2b1bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "guardar_output_como_pickle(user_to_train.replace(\".\",\"_\")+\".pickle\", clasificador_voz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa19974",
   "metadata": {},
   "outputs": [],
   "source": [
    "guardar_output_como_pickle(user_to_train.replace(\".\",\"_\")+\"_NAMES.pickle\", used_users)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eastern-humor",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "resident-clock",
   "metadata": {},
   "source": [
    "# TESTEO DE LA RED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "increased-passport",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CANTIDAD DE FRAMES MAL CLASIFICADOS\n",
    "cant_frames_mal_clasificados = 0\n",
    "\n",
    "#CANTIDAD DE FRAMES BIEN CLASIFICADOS\n",
    "cant_frames_bien_clasificados = 0\n",
    "\n",
    "\n",
    "\n",
    "#PARA CADA FRAME DE TESTEO, VEO LA PREDICCIÓN DE LA RED\n",
    "for i in range(0, df_MFCC_test.shape[0]):\n",
    "    \n",
    "    #Si la predicción del i-ésimo frame es igual al i-ésimo arreglo de personas que se identificaron\n",
    "    #está bien clasificado\n",
    "    if(np.array_equal(clasificador_voz.predict([df_MFCC_test[i]])[0], df_personas_test[i])):\n",
    "        cant_frames_bien_clasificados += 1\n",
    "    else:\n",
    "        cant_frames_mal_clasificados += 1\n",
    "\n",
    "        \n",
    "print(\"BIEN CLASIFICADOS = \",cant_frames_bien_clasificados)\n",
    "\n",
    "print(\"MAL CLASIFICADOS = \",cant_frames_mal_clasificados)\n",
    "\n",
    "\n",
    "print('score TRAIN:', clasificador_voz.score(df_MFCC, df_personas)) \n",
    "print('score TEST:', clasificador_voz.score(df_MFCC_test, df_personas_test)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "central-timber",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "substantial-least",
   "metadata": {},
   "source": [
    "### MATRIZ DE CONFUSIÓN DE FRAMES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "composed-triple",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Y_test_pred = clasificador_voz.predict(df_MFCC_test)\n",
    "\n",
    "fig = plt.figure(figsize=(15,6))\n",
    "\n",
    "ax1 = fig.add_subplot(121)\n",
    "skplt.metrics.plot_confusion_matrix(df_personas_test.argmax(axis=1), Y_test_pred.argmax(axis=1),\n",
    "                                    title=\"Frames Confusion Matrix\",\n",
    "                                    cmap=\"Oranges\",\n",
    "                                    ax=ax1)\n",
    "\n",
    "ax2 = fig.add_subplot(122)\n",
    "skplt.metrics.plot_confusion_matrix(df_personas_test.argmax(axis=1), Y_test_pred.argmax(axis=1),\n",
    "                                    normalize=True,\n",
    "                                    title=\"Frames % Confusion Matrix\",\n",
    "                                    cmap=\"Purples\",\n",
    "                                    ax=ax2);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fancy-klein",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "honest-steel",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#CANTIDAD DE FRAMES CLASIFICADOS\n",
    "cant_frames_mal_clasificados = 0\n",
    "cant_frames_bien_clasificados = 0\n",
    "\n",
    "#CANTIDAD DE PERSONAS CLASIFICADAS\n",
    "cant_personas_mal_clasificadas = 0\n",
    "cant_personas_bien_clasificadas = 0\n",
    "\n",
    "frames_persona = np.zeros((8,))\n",
    "\n",
    "\n",
    "#PARA CADA FRAME DE TESTEO, VEO LA PREDICCIÓN DE LA RED\n",
    "for i in range(0, df_MFCC_test.shape[0]):\n",
    "    \n",
    "    \n",
    "    #Si la predicción del i-ésimo frame es igual al i-ésimo arreglo de personas que se identificaron\n",
    "    #está bien clasificado  \n",
    "    \n",
    "    if(np.array_equal(clasificador_voz.predict([df_MFCC_test[i]])[0], df_personas_test[i])):\n",
    "        cant_frames_bien_clasificados += 1\n",
    "        \n",
    "    else:\n",
    "        cant_frames_mal_clasificados += 1\n",
    "\n",
    "\n",
    "        \n",
    "#VEO LA PREDICCIÓN POR PERSONAS DE LA RED\n",
    "for persona in range(0, cant_personas_train):\n",
    "\n",
    "    #RECORRO LOS FRAMES DE TESTEO PARA VALIDAR LA RED\n",
    "    for frame in range(0, df_MFCC_test.shape[0]):\n",
    "  \n",
    "        #ME QUEDO CON LOS FRAMES QUE IDENTIFIQUE QUE SEAN DE LA PERSONA\n",
    "        #ESTOS SON LOS FRAMES IDENTIFICADOS QUE USO PARA TESTEO (VALIDACION)\n",
    "        if(df_personas_test[frame][persona] == 1):\n",
    "      \n",
    "            #RECORRO DE 0 a 7, 8 PERSONAS. PARA SABER SI EL OUTPUT QUE ME DIO LA RED\n",
    "            #ES EL DE LA PERSONA QUE YO NECESITO\n",
    "            for j in range(0, cant_personas_train):\n",
    "        \n",
    "                #SI ES EL DE ESA PERSONA, SETEO QUE LE IDENTIFIQUE UN FRAME\n",
    "                if(clasificador_voz.predict([df_MFCC_test[frame]])[0][j] == 1):\n",
    "                    frames_persona[j] += 1\n",
    "  \n",
    "    #YA RECORRI TODOS LOS FRAMES DE TESTEO, AHORA ME FIJO CON QUE PERSONA SE\n",
    "    #IDENTIFICARON LA MAYOR CANTIDAD DE FRAMES\n",
    "\n",
    "    persona_mas_frames = 0\n",
    "    cant_frames_mayor_de_persona_identificada = 0\n",
    "\n",
    "    #VEO EL VECTOR DONDE ME GUARDE LA CANTIDAD DE FRAMES QUE IDENTIFIQUE PARA\n",
    "    #CADA PERSONA\n",
    "    for persona_identificacion in range(0,cant_personas_train):\n",
    "\n",
    "        if(frames_persona[persona_identificacion] > cant_frames_mayor_de_persona_identificada):\n",
    "            cant_frames_mayor_de_persona_identificada = frames_persona[persona_identificacion]\n",
    "            persona_mas_frames = persona_identificacion\n",
    "\n",
    "    #YA TENGO QUE PERSONA FUE CON LA QUE MAS FRAMES SE IDENTIFICARON VEO SI ES LA\n",
    "    #QUE TENIA QUE SER\n",
    "\n",
    "    if(persona_mas_frames == persona):\n",
    "        cant_personas_bien_clasificadas += 1\n",
    "    else:\n",
    "        cant_personas_mal_clasificadas += 1\n",
    "\n",
    "\n",
    "\n",
    "    print(\"ERA LA PERSONA \",persona,\" IDENTIFIQUE :\",frames_persona)\n",
    "    frames_persona = np.zeros((8,))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"FRAMES TEST BIEN CLASIFICADOS = \",cant_frames_bien_clasificados)\n",
    "print(\"FRAMES TEST MAL CLASIFICADOS = \",cant_frames_mal_clasificados)\n",
    "print(\"PERSONAS TEST BIEN CLASIFICADAS = \",cant_personas_bien_clasificadas)\n",
    "print(\"PERSONAS TEST MAL CLASIFICADAS = \",cant_personas_mal_clasificadas)\n",
    "\n",
    "\n",
    "print('score FRAMES TRAIN:', clasificador_voz.score(df_MFCC, df_personas)) \n",
    "print('score FRAMES TEST:', clasificador_voz.score(df_MFCC_test, df_personas_test)) \n",
    "print('score PERSONAS TEST:', cant_personas_train/cant_personas_bien_clasificadas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "roman-digest",
   "metadata": {},
   "outputs": [],
   "source": [
    "cant_frames_totales = cant_frames_test+cant_frames_train\n",
    "print('cant frames test  ',cant_frames_test)\n",
    "print('cant frames train ',cant_frames_train)\n",
    "print('cant frames total ',cant_frames_totales)\n",
    "\n",
    "print('distribucion train {:.2%}'.format(cant_frames_train/cant_frames_totales))\n",
    "print('distribucion test  {:.2%}'.format(cant_frames_test/cant_frames_totales))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amino-studio",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d9b802",
   "metadata": {},
   "source": [
    "# ACA YA FINALIZA EL MODELO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8582aa3",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cutting-vision",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#Testeo de cada audio que deje para test\n",
    "\n",
    "\n",
    "filas_dataframe = [[None,None,None,None]]\n",
    "\n",
    "#Persona de la que leo los features\n",
    "persona_actual = 0\n",
    "\n",
    "\n",
    "#Auxiliar para saber la cantidad de frames totales para train\n",
    "cant_frames_test = 0\n",
    "\n",
    "\n",
    "#CANTIDAD TOTAL DE FRAMES CLASIFICADOS\n",
    "cant_total_frames_mal_clasificados = 0\n",
    "cant_total_frames_bien_clasificados = 0\n",
    "\n",
    "#CANTIDAD DE AUDIOS CLASIFICADOS\n",
    "cant_audios_mal_clasificados = 0\n",
    "cant_audios_bien_clasificados = 0\n",
    "\n",
    "\n",
    "#Recorro las carpetas de la región del dialecto, donde cada subcarpeta es una persona.\n",
    "for persona in used_users:\n",
    "\n",
    "    \n",
    "    #Persona de la que leo los features\n",
    "    persona_actual += 1\n",
    "\n",
    "    \n",
    "    #Contador de frames leidos de los archivos de la persona actual\n",
    "    cant_frames_persona_actual = 0\n",
    "\n",
    "    \n",
    "    #Carpeta de los archivos de audio de cada persona\n",
    "    for archivo_voz in diccionario_audios_test.get(persona):\n",
    "\n",
    "    \n",
    "        file = open('temp_audio.wav', 'wb+')\n",
    "        bucket.get_blob(archivo_voz).download_to_file(file)\n",
    "        file.close()\n",
    "        \n",
    "        #Extraigo los MFCC de un audio\n",
    "        mfcc = loadMfccArchivo(file.name)\n",
    "        \n",
    "        print(\"PERSONA \",persona_actual, \" --------------------- ARCHIVO \",archivo_voz)\n",
    "    \n",
    "\n",
    "        \n",
    "        #Actualmente en el array tengo n filas x m columnas, donde n es la cantidad de MFCC y m la cantidad de frames\n",
    "        #Traspongo la matriz para quedarme con los features de un determinado frame, es decir n cantidad de MFCC\n",
    "        #columnas x m cantidad de frames filas\n",
    "        mfcc = np.transpose(mfcc)\n",
    "        \n",
    "        \n",
    "        #Para agregar información temporal y fonética (velocidad, aceleración, entre otras) a los coeficientes\n",
    "        #agrego los delta de cada feature y los concateno al array en formato de columnas\n",
    "        mfcc = np.append(mfcc, librosa.feature.delta(mfcc), axis = 1)\n",
    "        \n",
    "        \n",
    "        \n",
    "        #Contador de frames leidos de los archivos de la persona actual\n",
    "        cant_frames_audio_actual = mfcc.shape[0]\n",
    "        \n",
    "\n",
    "        \n",
    "        #CANTIDAD DE FRAMES CLASIFICADOS DEL AUDIO ACTUAL\n",
    "        cant_frames_audio_actual_mal_clasificados = 0\n",
    "        cant_frames_audio_actual_bien_clasificados = 0\n",
    "        \n",
    "        \n",
    "        #ARRAY PARA ASIGNAR LAS PREDICCIONES DE CADA FRAME DEL AUDIO ACTUAL, A PERSONAS\n",
    "        predicciones_frames_personas = np.zeros((cant_personas_train,))\n",
    "\n",
    "        \n",
    "        #Recorro todos los frames del audio\n",
    "        for frame in mfcc:\n",
    "            \n",
    "            prediccion = clasificador_voz.predict([frame])[0]\n",
    "            \n",
    "            if prediccion[persona_actual-1] == 1:\n",
    "                cant_frames_audio_actual_bien_clasificados+=1\n",
    "            else:\n",
    "                cant_frames_audio_actual_mal_clasificados+=1\n",
    "         \n",
    "            #RECORRO EL FRAME PARA SABER A QUE PERSONA CORRESPONDE LA PREDICCION\n",
    "            for i in range(0, prediccion.shape[0]):\n",
    "        \n",
    "                #SI ES EL DE ESA PERSONA, SETEO QUE LE IDENTIFIQUE UN FRAME, Y NO RECORRO MÁS\n",
    "                if(prediccion[i] == 1):\n",
    "                    predicciones_frames_personas[i] += 1\n",
    "                    break\n",
    "                 \n",
    "                \n",
    "        #PERSONA IDENTIFICADA. INDICE MAXIMO +1 \n",
    "        persona_identificada = predicciones_frames_personas.argmax()+1\n",
    "        \n",
    "        \n",
    "        if(persona_identificada == persona_actual):\n",
    "            cant_audios_bien_clasificados += 1\n",
    "        else:\n",
    "            cant_audios_mal_clasificados += 1\n",
    "\n",
    "        \n",
    "        \n",
    "        cant_total_frames_mal_clasificados += cant_frames_audio_actual_mal_clasificados\n",
    "        cant_total_frames_bien_clasificados += cant_frames_audio_actual_bien_clasificados\n",
    "            \n",
    "        filas_dataframe.append([archivo_voz, persona_actual, cant_frames_audio_actual, predicciones_frames_personas])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f005d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Audios BIEN clasificados = \",cant_audios_bien_clasificados)\n",
    "print(\"Audios MAL clasificados  = \",cant_audios_mal_clasificados)\n",
    "print(\"Frames BIEN clasificados = \",cant_total_frames_bien_clasificados)\n",
    "print(\"Frames MAL clasificados  = \",cant_total_frames_mal_clasificados)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "driving-ratio",
   "metadata": {},
   "source": [
    "## Creamos el DF del resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "significant-reader",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "columns = ['Audio', 'Persona del audio', 'Cantidad de frames', 'Predicciones en frames']\n",
    "\n",
    "\n",
    "df_resultado = pd.DataFrame(filas_dataframe, columns=columns)\n",
    "\n",
    "df_resultado.drop(0, inplace=True)\n",
    "\n",
    "df_resultado"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "occupational-destiny",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "flexible-interference",
   "metadata": {},
   "source": [
    "### MATRIZ DE CONFUSIÓN DE LOS AUDIOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dress-essex",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import classification_report\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# print(classification_report(Y_test_real_audio, Y_test_pred_audio, digits=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "limiting-hunger",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Y_test_pred_audio = []\n",
    "Y_test_real_audio = df_resultado['Persona del audio'].to_numpy()\n",
    "\n",
    "#Tomo la columna predicción en frames de cada audio y sumo 1 al argmax para que las personas sean de 1 a N\n",
    "for pred in df_resultado['Predicciones en frames']:\n",
    "    Y_test_pred_audio.append(pred.argmax()+1)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "fig = plt.figure(figsize=(15,6))\n",
    "\n",
    "ax1 = fig.add_subplot(121)\n",
    "skplt.metrics.plot_confusion_matrix(Y_test_real_audio, Y_test_pred_audio,\n",
    "                                    title=\"Audios Confusion Matrix\",\n",
    "                                    cmap=\"Oranges\",\n",
    "                                    ax=ax1)\n",
    "\n",
    "ax2 = fig.add_subplot(122)\n",
    "skplt.metrics.plot_confusion_matrix(Y_test_real_audio, Y_test_pred_audio,\n",
    "                                    normalize=True,\n",
    "                                    title=\"Audios % Confusion Matrix\",\n",
    "                                    cmap=\"Purples\",\n",
    "                                    ax=ax2);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4352686",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_actu = pd.Series(Y_test_real_audio, name='Actual')\n",
    "y_pred = pd.Series(Y_test_pred_audio, name='Predicted')\n",
    "df_confusion = pd.crosstab(y_actu, y_pred)\n",
    "print(\"CONFUSION MATRIX FOR TRAIN AUDIOS\\n\\n\")\n",
    "print(df_confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "published-bolivia",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "audios_accuracy = metrics.accuracy_score(Y_test_real_audio, Y_test_pred_audio)\n",
    "audios_precision = metrics.precision_score(Y_test_real_audio, Y_test_pred_audio, average='weighted')\n",
    "\n",
    "\n",
    "print(\"AUDIOS ACCURACY TEST = {:.2%}\".format(audios_accuracy))\n",
    "print(\"AUDIOS PRECISION TEST = {:.2%}\".format(audios_precision))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "received-heavy",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "current-adelaide",
   "metadata": {},
   "source": [
    "## CONCLUSIONES INICIALES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proof-yesterday",
   "metadata": {},
   "source": [
    "A nivel audios hay un excelente resultado.\n",
    "\n",
    "Notamos una gran diferencia entre la asignación de frames de un audio, entre las dos primeras personas con mayor asignación de frames. Es decir, asigna muchos más frames a la primera persona que a la segunda.\n",
    "\n",
    "A la persona identificada en primer lugar, le asigna una gran cantidad de frames del audio respecto al total del mismo.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "discrete-engineering",
   "metadata": {},
   "source": [
    "#### "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "color-moldova",
   "metadata": {},
   "source": [
    "A partir de estas conclusiones es que vamos a determinar un umbral de diferencia entre la asignación de frames del total del audio y de diferencia entre la primera persona y la segunda para poder filtrar los \"impostores\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exact-diamond",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "genuine-house",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def calcular_desvio_entre_frames_asignados_a_personas(frames_asignados_a_personas):\n",
    "    \n",
    "    persona_max_asignaciones = 1\n",
    "    frames_persona_max_asignaciones = frames_asignados_a_personas[0]\n",
    "    \n",
    "    persona_segunda_max_asignaciones = 0\n",
    "    frames_persona_segunda_max_asignaciones = 0\n",
    "    \n",
    "    \n",
    "    for i in range(1,len(frames_asignados_a_personas)):\n",
    "        \n",
    "        if (frames_asignados_a_personas[i]>frames_persona_max_asignaciones):\n",
    "            \n",
    "            persona_segunda_max_asignaciones = persona_max_asignaciones\n",
    "            frames_persona_segunda_max_asignaciones = frames_persona_max_asignaciones\n",
    "            \n",
    "            persona_max_asignaciones = i+1\n",
    "            frames_persona_max_asignaciones = frames_asignados_a_personas[i]\n",
    "            \n",
    "            \n",
    "            \n",
    "        elif (frames_asignados_a_personas[i]>frames_persona_segunda_max_asignaciones):\n",
    "            \n",
    "            persona_segunda_max_asignaciones = i+1\n",
    "            frames_persona_segunda_max_asignaciones = frames_asignados_a_personas[i] \n",
    "                       \n",
    "     \n",
    "    desvio_frames_primer_y_segunda_persona = (frames_persona_max_asignaciones/frames_persona_segunda_max_asignaciones)-1\n",
    "    \n",
    "    \n",
    "    return desvio_frames_primer_y_segunda_persona\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plain-cargo",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "columns = ['Audio', 'Persona del audio', 'Cantidad de frames', 'Predicciones en frames']\n",
    "\n",
    "\n",
    "df_resultado = pd.DataFrame(filas_dataframe, columns=columns)\n",
    "\n",
    "\n",
    "df_resultado.drop(0, inplace=True)\n",
    "\n",
    "\n",
    "df_resultado['Persona del audio'] = df_resultado['Persona del audio'].astype(int)\n",
    "\n",
    "\n",
    "df_resultado['Frames bien clasificados'] = df_resultado.apply(lambda row : \n",
    "                                                              row['Predicciones en frames'][row['Persona del audio']-1]\n",
    "                                                              .astype(int), axis=1)\n",
    "\n",
    "\n",
    "df_resultado['Frames mal clasificados'] = df_resultado.apply(lambda row : \n",
    "                                                              (row['Predicciones en frames'].sum() -\n",
    "                                                               row['Frames bien clasificados'])\n",
    "                                                              .astype(int), axis=1)\n",
    "\n",
    "\n",
    "df_resultado['Frames no clasificados'] = df_resultado.apply(lambda row : \n",
    "                                                              (row['Cantidad de frames'] - \n",
    "                                                               row['Predicciones en frames'].sum())\n",
    "                                                              .astype(int), axis=1)\n",
    "\n",
    "\n",
    "df_resultado['Porcentaje no clasificados'] = df_resultado.apply(lambda row : \n",
    "                                                              math.ceil(row['Frames no clasificados'] / \n",
    "                                                               row['Cantidad de frames']*100)\n",
    "                                                              , axis=1)\n",
    "\n",
    "\n",
    "df_resultado['Persona predicha'] = df_resultado['Predicciones en frames'].apply(lambda x : x.argmax()+1)\n",
    "\n",
    "\n",
    "df_resultado['Prediccion correcta'] = df_resultado['Persona predicha'] == df_resultado['Persona del audio']\n",
    "\n",
    "\n",
    "df_resultado['Persona segunda max frames'] = (df_resultado['Predicciones en frames']\n",
    "                                              .apply(lambda x : np.argwhere(x==np.sort(x)[-2])[0][0]+1))\n",
    "\n",
    "\n",
    "df_resultado['Frames persona segunda max frames'] = df_resultado.apply(lambda row : \n",
    "                                                            (row['Predicciones en frames'][row['Persona segunda max frames']-1])\n",
    "                                                              .astype(int)\n",
    "                                                              , axis=1)\n",
    "\n",
    "\n",
    "df_resultado['Frames Persona Max Frames'] = df_resultado.apply(lambda row: int(row['Predicciones en frames']\n",
    "                                                               [row['Persona predicha']-1]), axis=1)\n",
    "\n",
    "\n",
    "df_resultado['Porcentaje diferencia primera persona con segunda'] = (((df_resultado['Frames Persona Max Frames'] /\n",
    "                                                                     df_resultado['Frames persona segunda max frames'])-1)*100).apply(lambda x: math.ceil(x))\n",
    "\n",
    "\n",
    "df_resultado['Porcentaje max sobre total'] = (df_resultado['Frames Persona Max Frames'] / df_resultado['Cantidad de frames'])*100\n",
    "\n",
    "df_resultado['Porcentaje max sobre total sin blancos'] = (df_resultado['Frames Persona Max Frames'] / (df_resultado['Cantidad de frames']-df_resultado['Frames no clasificados']))*100\n",
    "\n",
    "\n",
    "df_resultado['Desvio'] = df_resultado['Predicciones en frames'].apply(lambda x : calcular_desvio_entre_frames_asignados_a_personas(x))\n",
    "\n",
    "\n",
    "df_resultado\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "destroyed-action",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "requested-amplifier",
   "metadata": {},
   "source": [
    "# Analisis diferencias primera persona identificada vs segunda"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bored-corporation",
   "metadata": {},
   "source": [
    "Para hacer un poco más riguroso el análisis, tomemos el promedio de los 5 porcentajes más bajos de diferencia, ya que solemos tener una diferencia MUY amplia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adolescent-country",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "df_resultado.sort_values(by='Porcentaje diferencia primera persona con segunda'\n",
    "                        )['Porcentaje diferencia primera persona con segunda'][:5].values.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "looking-petite",
   "metadata": {},
   "source": [
    "Dado este resultado, podemos establecer un umbral de \"validación\" del 50% para hacer una validación más rigurosa y poder filtrar posibles impostores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jewish-spectacular",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "postal-massage",
   "metadata": {},
   "source": [
    "## Análisis porcentajes del total del audio asignado a la persona identificada"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stuffed-prediction",
   "metadata": {},
   "source": [
    "Para hacer un poco más riguroso el análisis, tomemos el promedio de los 5 porcentajes más bajos de asignación del total de frames del audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lesbian-oracle",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_resultado.sort_values(by='Porcentaje max sobre total'\n",
    "                        )['Porcentaje max sobre total'][:5].values.mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decent-standing",
   "metadata": {},
   "source": [
    "Dado este resultado, podemos establecer un umbral de \"validación\" del 30% para hacer una validación más rigurosa y poder filtrar posibles impostores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "illegal-prevention",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "moral-finance",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validar_prediccion(predicciones_frames, porcentaje_sobre_total):\n",
    "    desvio = calcular_desvio_entre_frames_asignados_a_personas(predicciones_frames)\n",
    "    \n",
    "    validacion_desvio = (1 if desvio>=.5 else (.5 if desvio>=.25 else 0))\n",
    "    validacion_porcentaje = (1 if np.ceil(porcentaje_sobre_total)>=40 else 0)\n",
    "    \n",
    "    return min(validacion_desvio, validacion_porcentaje)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "french-weapon",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incorrect-easter",
   "metadata": {},
   "source": [
    "## Probemos resultados con este validador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defensive-secret",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_resultado['Validar Asignacion'] = df_resultado.apply(lambda row: \n",
    "                                                        validar_prediccion(row['Predicciones en frames'], \n",
    "                                                                              row['Porcentaje max sobre total'])\n",
    "                                                        , axis=1)\n",
    "\n",
    "\n",
    "df_resultado['prediccion_validada'] = df_resultado.apply(lambda row: row['Persona predicha'] if \n",
    "                                                         row['Validar Asignacion'] else -1, axis=1)\n",
    "\n",
    "\n",
    "df_resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alternative-frontier",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_resultado[df_resultado['prediccion_validada']==-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "instructional-sherman",
   "metadata": {},
   "outputs": [],
   "source": [
    "# la columna predicción en frames de cada audio y sumo 1 al argmax para que las personas sean de 1 a N\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "fig = plt.figure(figsize=(15,6))\n",
    "\n",
    "ax1 = fig.add_subplot(121)\n",
    "skplt.metrics.plot_confusion_matrix(df_resultado['Persona del audio'].to_numpy(), \n",
    "                                    df_resultado['prediccion_validada'].to_numpy(),\n",
    "                                    title=\"Audios Confusion Matrix\",\n",
    "                                    cmap=\"Oranges\",\n",
    "                                    ax=ax1)\n",
    "\n",
    "ax2 = fig.add_subplot(122)\n",
    "skplt.metrics.plot_confusion_matrix(df_resultado['Persona del audio'].to_numpy(), \n",
    "                                    df_resultado['prediccion_validada'].to_numpy(),\n",
    "                                    normalize=True,\n",
    "                                    title=\"Audios % Confusion Matrix\",\n",
    "                                    cmap=\"Blues\",\n",
    "                                    ax=ax2);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f323b02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_actu = pd.Series(df_resultado['Persona del audio'], name='Actual')\n",
    "y_pred = pd.Series(df_resultado['prediccion_validada'], name='Predicted')\n",
    "df_confusion = pd.crosstab(y_actu, y_pred)\n",
    "print(\"CONFUSION MATRIX FOR TEST AUDIOS\\n\\n\")\n",
    "print(df_confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "furnished-present",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(print(\"Predicciones de audios de personas conocidas por la red con validaciones \\n\\n\"))\n",
    "\n",
    "print(df_resultado[['Persona del audio','Prediccion correcta','Validar Asignacion']].groupby(['Prediccion correcta','Validar Asignacion']).count().reset_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dramatic-cholesterol",
   "metadata": {},
   "outputs": [],
   "source": [
    "audios_accuracy_validador = metrics.accuracy_score(df_resultado['Persona del audio'].to_numpy(),\n",
    "                                                   df_resultado['prediccion_validada'].to_numpy())\n",
    "\n",
    "audios_precision_validador = metrics.precision_score(df_resultado['Persona del audio'].to_numpy(), \n",
    "                                                     df_resultado['prediccion_validada'].to_numpy(), average='weighted')\n",
    "\n",
    "\n",
    "\n",
    "print(\"AUDIOS ACCURACY = {:.2%}\".format(audios_accuracy_validador))\n",
    "print(\"AUDIOS PRECISION = {:.2%}\".format(audios_precision_validador))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "global-merchant",
   "metadata": {},
   "source": [
    "Si bien se reduce el accuracy , logramos un 100% de precisión. Es decir, que nunca el clasificador considerará válida una predicción, si no está realmente seguro (no hay FP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adequate-sterling",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "residential-pearl",
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#Testeo de cada audio que deje para test\n",
    "\n",
    "\n",
    "filas_dataframe_desconocido = [[None,None,None,None]]\n",
    "\n",
    "\n",
    "#Auxiliar para saber la cantidad de frames totales para train\n",
    "cant_frames_persona_desconocida = 0\n",
    "\n",
    "\n",
    "\n",
    "#Recorro las carpetas de la región del dialecto, donde cada subcarpeta es una persona.\n",
    "for persona in not_used_users:\n",
    "\n",
    "    print(persona)\n",
    "    #Carpeta de los archivos de audio de cada persona\n",
    "    for archivo_voz in diccionario_audios_not_used_users.get(persona):\n",
    "        \n",
    "        print(\"PERSONA \",persona, \" --------------------- ARCHIVO \",archivo_voz)\n",
    "    \n",
    "        file = open('temp_audio.wav', 'wb+')\n",
    "        bucket.get_blob(archivo_voz).download_to_file(file)\n",
    "        file.close()\n",
    "        \n",
    "        #Extraigo los MFCC de un audio\n",
    "        mfcc = loadMfccArchivo(file.name)\n",
    "        \n",
    "        \n",
    "        #Actualmente en el array tengo n filas x m columnas, donde n es la cantidad de MFCC y m la cantidad de frames\n",
    "        #Traspongo la matriz para quedarme con los features de un determinado frame, es decir n cantidad de MFCC\n",
    "        #columnas x m cantidad de frames filas\n",
    "        mfcc = np.transpose(mfcc)\n",
    "        \n",
    "        \n",
    "        #Para agregar información temporal y fonética (velocidad, aceleración, entre otras) a los coeficientes\n",
    "        #agrego los delta de cada feature y los concateno al array en formato de columnas\n",
    "        mfcc = np.append(mfcc, librosa.feature.delta(mfcc), axis = 1)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        #Contador de frames leidos de los archivos de la persona actual\n",
    "        cant_frames_audio_actual = mfcc.shape[0]\n",
    "        \n",
    "        cant_frames_persona_desconocida+=cant_frames_audio_actual\n",
    "        \n",
    "    \n",
    "        \n",
    "        #CANTIDAD DE FRAMES CLASIFICADOS DEL AUDIO ACTUAL\n",
    "        cant_frames_audio_actual_mal_clasificados = 0\n",
    "        \n",
    "        \n",
    "        #ARRAY PARA ASIGNAR LAS PREDICCIONES DE CADA FRAME DEL AUDIO ACTUAL, A PERSONAS\n",
    "        predicciones_frames_personas = np.zeros((cant_personas_train,))\n",
    "\n",
    "        \n",
    "        #Recorro todos los frames del audio\n",
    "        for frame in mfcc:\n",
    "            \n",
    "            prediccion = clasificador_voz.predict([frame])[0]\n",
    "            \n",
    "            cant_frames_audio_actual_mal_clasificados+=1\n",
    "         \n",
    "            #RECORRO EL FRAME PARA SABER A QUE PERSONA CORRESPONDE LA PREDICCION\n",
    "            for i in range(0, prediccion.shape[0]):\n",
    "        \n",
    "                #SI ES EL DE ESA PERSONA, SETEO QUE LE IDENTIFIQUE UN FRAME, Y NO RECORRO MÁS\n",
    "                if(prediccion[i] == 1):\n",
    "                    predicciones_frames_personas[i] += 1\n",
    "                    break\n",
    "                 \n",
    "                \n",
    "        #PERSONA IDENTIFICADA. INDICE MAXIMO +1 \n",
    "        persona_identificada = predicciones_frames_personas.argmax()+1\n",
    "        \n",
    "\n",
    "            \n",
    "        filas_dataframe_desconocido.append([persona+\"\\\\\"+archivo_voz, 8, cant_frames_audio_actual, predicciones_frames_personas])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "molecular-bishop",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['Audio', 'Persona del audio', 'Cantidad de frames', 'Predicciones en frames']\n",
    "\n",
    "\n",
    "df_resultado_desconocido = pd.DataFrame(filas_dataframe_desconocido, columns=columns)\n",
    "\n",
    "\n",
    "df_resultado_desconocido.drop(0, inplace=True)\n",
    "\n",
    "df_resultado_desconocido['Nombre_persona'] = df_resultado_desconocido['Audio'].apply(lambda x: x.split('\\\\')[0])\n",
    "\n",
    "df_resultado_desconocido['Persona del audio'] = -1\n",
    "\n",
    "\n",
    "\n",
    "df_resultado_desconocido['Frames bien clasificados'] = 0\n",
    "\n",
    "\n",
    "df_resultado_desconocido['Frames mal clasificados'] = df_resultado_desconocido.apply(lambda row: row['Predicciones en frames'].sum(), axis=1)\n",
    "\n",
    "\n",
    "df_resultado_desconocido['Frames no clasificados'] = df_resultado_desconocido.apply(lambda row : \n",
    "                                                              (row['Cantidad de frames'] - \n",
    "                                                               row['Predicciones en frames'].sum())\n",
    "                                                              .astype(int), axis=1)\n",
    "\n",
    "\n",
    "df_resultado_desconocido['Porcentaje no clasificados'] = df_resultado_desconocido.apply(lambda row : \n",
    "                                                              math.ceil(row['Frames no clasificados'] / \n",
    "                                                               row['Cantidad de frames']*100)\n",
    "                                                              , axis=1)\n",
    "\n",
    "\n",
    "df_resultado_desconocido['Persona predicha'] = df_resultado_desconocido['Predicciones en frames'].apply(lambda x : x.argmax()+1)\n",
    "\n",
    "\n",
    "df_resultado_desconocido['Prediccion correcta'] = df_resultado_desconocido['Persona predicha'] == df_resultado_desconocido['Persona del audio']\n",
    "\n",
    "\n",
    "df_resultado_desconocido['Persona segunda max frames'] = (df_resultado_desconocido['Predicciones en frames']\n",
    "                                              .apply(lambda x : np.argwhere(x==np.sort(x)[-2])[0][0]+1))\n",
    "\n",
    "\n",
    "df_resultado_desconocido['Frames persona segunda max frames'] = df_resultado_desconocido.apply(lambda row : \n",
    "                                                            (row['Predicciones en frames'][row['Persona segunda max frames']-1])\n",
    "                                                              .astype(int)\n",
    "                                                              , axis=1)\n",
    "\n",
    "\n",
    "df_resultado_desconocido['Frames Persona Max Frames'] = df_resultado_desconocido.apply(lambda row: int(row['Predicciones en frames']\n",
    "                                                               [row['Persona predicha']-1]), axis=1)\n",
    "\n",
    "\n",
    "df_resultado_desconocido['Porcentaje diferencia primera persona con segunda'] = (((df_resultado_desconocido['Frames Persona Max Frames'] /\n",
    "                                                                     df_resultado_desconocido['Frames persona segunda max frames'])-1)*100).apply(lambda x: math.ceil(x))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df_resultado_desconocido['Porcentaje max sobre total'] = (df_resultado_desconocido['Frames Persona Max Frames'] / df_resultado_desconocido['Cantidad de frames'])*100\n",
    "\n",
    "df_resultado_desconocido['Porcentaje max sobre total sin blancos'] = (df_resultado_desconocido['Frames Persona Max Frames'] / (df_resultado_desconocido['Cantidad de frames']-df_resultado_desconocido['Frames no clasificados']))*100\n",
    "\n",
    "df_resultado_desconocido['Validar Asignacion'] = df_resultado_desconocido.apply(lambda row: validar_prediccion(row['Predicciones en frames'], row['Porcentaje max sobre total']), axis=1)\n",
    "\n",
    "\n",
    "\n",
    "df_resultado_desconocido['Desvio'] = df_resultado_desconocido['Predicciones en frames'].apply(lambda x : calcular_desvio_entre_frames_asignados_a_personas(x))\n",
    "\n",
    "\n",
    "df_resultado_desconocido\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "administrative-alloy",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "persona_actual = 0\n",
    "filas_personas_nombres = [[None,None]]\n",
    "\n",
    "for persona in used_users:\n",
    "\n",
    "    \n",
    "    #Persona de la que leo los features\n",
    "    persona_actual += 1\n",
    "\n",
    "    filas_personas_nombres.append([persona_actual, persona])\n",
    "    \n",
    "personas_nombres = pd.DataFrame(filas_personas_nombres, columns=['Persona predicha', 'Nombre_persona_predicha']).dropna()    \n",
    "personas_nombres\n",
    "\n",
    "print(personas_nombres)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tutorial-target",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unable-slovak",
   "metadata": {},
   "source": [
    "## Analicemos resultados de los impostores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amazing-clearing",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_resultado_desconocido['prediccion_validada'] = df_resultado_desconocido.apply(lambda row: row['Persona predicha'] if \n",
    "                                                         row['Validar Asignacion'] else -1, axis=1)\n",
    "    \n",
    "    \n",
    "    \n",
    "fig = plt.figure(figsize=(15,6))\n",
    "\n",
    "ax1 = fig.add_subplot(121)\n",
    "skplt.metrics.plot_confusion_matrix(df_resultado_desconocido['Persona del audio'].to_numpy(), \n",
    "                                    df_resultado_desconocido['prediccion_validada'].to_numpy(),\n",
    "                                    title=\"Audios Confusion Matrix\",\n",
    "                                    cmap=\"Oranges\",\n",
    "                                    ax=ax1)\n",
    "\n",
    "ax2 = fig.add_subplot(122)\n",
    "skplt.metrics.plot_confusion_matrix(df_resultado_desconocido['Persona del audio'].to_numpy(), \n",
    "                                    df_resultado_desconocido['prediccion_validada'].to_numpy(),\n",
    "                                    normalize=True,\n",
    "                                    title=\"Audios % Confusion Matrix\",\n",
    "                                    cmap=\"Blues\",\n",
    "                                    ax=ax2);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365c0636",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_actu = pd.Series(df_resultado_desconocido['Persona del audio'], name='Actual')\n",
    "y_pred = pd.Series(df_resultado_desconocido['prediccion_validada'], name='Predicted')\n",
    "df_confusion = pd.crosstab(y_actu, y_pred)\n",
    "print(\"CONFUSION MATRIX FOR IMPOSTORS AUDIOS\\n\\n\")\n",
    "print(df_confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956549d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Predicciones de audios de impostores\\n\\n\")\n",
    "print(df_resultado_desconocido[['Prediccion correcta', 'Validar Asignacion', 'Audio']].groupby(['Prediccion correcta', 'Validar Asignacion']).count().reset_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lovely-figure",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_resultado_desconocido[['Prediccion correcta', 'Validar Asignacion', 'Audio']].groupby(['Prediccion correcta', 'Validar Asignacion']).count().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "specific-export",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "audios_accuracy_validador = metrics.accuracy_score(df_resultado_desconocido['Persona del audio'].to_numpy(),\n",
    "                                                   df_resultado_desconocido['prediccion_validada'].to_numpy())\n",
    "\n",
    "audios_precision_validador = metrics.precision_score(df_resultado_desconocido['Persona del audio'].to_numpy(), \n",
    "                                                     df_resultado_desconocido['prediccion_validada'].to_numpy(), average='weighted')\n",
    "\n",
    "\n",
    "\n",
    "print(\"AUDIOS IMPOSTORES ACCURACY = {:.2%}\".format(audios_accuracy_validador))\n",
    "print(\"AUDIOS IMPOSTORES PRECISION = {:.2%}\".format(audios_precision_validador))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9945299d",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5c518e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('score FRAMES TRAIN:', clasificador_voz.score(df_MFCC, df_personas)) \n",
    "print('score FRAMES TEST:', clasificador_voz.score(df_MFCC_test, df_personas_test)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f104fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cant_frames_totales = cant_frames_test+cant_frames_train\n",
    "print('cant frames test  ',cant_frames_test)\n",
    "print('cant frames train ',cant_frames_train)\n",
    "print('cant frames total ',cant_frames_totales)\n",
    "\n",
    "print('distribucion train {:.2}'.format(cant_frames_train/cant_frames_totales))\n",
    "print('distribucion test  {:.2}'.format(cant_frames_test/cant_frames_totales))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9f409e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"BIEN CLASIFICADOS = \",cant_frames_bien_clasificados)\n",
    "\n",
    "print(\"MAL CLASIFICADOS = \",cant_frames_mal_clasificados)\n",
    "\n",
    "\n",
    "print('score TRAIN:', clasificador_voz.score(df_MFCC, df_personas)) \n",
    "print('score TEST:', clasificador_voz.score(df_MFCC_test, df_personas_test)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3c4eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "audios_accuracy = metrics.accuracy_score(Y_test_real_audio, Y_test_pred_audio)\n",
    "audios_precision = metrics.precision_score(Y_test_real_audio, Y_test_pred_audio, average='weighted')\n",
    "\n",
    "\n",
    "print(\"AUDIOS ACCURACY TEST = {:.2%}\".format(audios_accuracy))\n",
    "print(\"AUDIOS PRECISION TEST = {:.2%}\".format(audios_precision))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
